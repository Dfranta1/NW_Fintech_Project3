{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c998d54-f78f-45e0-abaa-02a6c7bf6485",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install newsapi-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fc1aa5-c893-47ff-b1a3-d93d6d045c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from newsapi import NewsApiClient\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d7ea78-cece-4cbf-8b38-d10feaa2988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk and vader imports\n",
    "\n",
    "import nltk as nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5424f6-49a6-4eb5-a134-02d775a35a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"news_api\")\n",
    "#api_key = os.getenv(news_api)\n",
    "api_key\n",
    "print(api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c4b632-f2c5-46c2-bce2-b0fbca8c4459",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsapi = NewsApiClient(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f55233-aa9a-4f15-82f2-4073111adda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_x_headlines = newsapi.get_top_headlines(q=\"stock name\", language=\"en\", country=\"us\")\n",
    "stock_x_headlines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d353e6a7-a6d7-46ed-aeec-9124559285d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print total articles\n",
    "print(f\"Total articles about stock x: {stock_x_headlines['totalResults']}\")\n",
    "\n",
    "# Show sample article\n",
    "stock_x[\"articles\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4895c5fb-9b08-4b62-8194-3760cf470ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the response dictionary to a DataFrame\n",
    "stock_x_df = pd.DataFrame.from_dict(stock_x_headlines[\"articles\"])\n",
    "\n",
    "stock_x_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd632912-d565-47e8-bec4-8ea558e01b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the stock_x sentiment scores DataFrame\n",
    "stock_x_sentiments = []\n",
    "\n",
    "for article in stock_x_news[\"articles\"]:\n",
    "    try:\n",
    "        text = article[\"content\"]\n",
    "        date = article[\"publishedAt\"][:10]\n",
    "        sentiment = analyzer.polarity_scores(text)\n",
    "        compound = sentiment[\"compound\"]\n",
    "        pos = sentiment[\"pos\"]\n",
    "        neu = sentiment[\"neu\"]\n",
    "        neg = sentiment[\"neg\"]\n",
    "        \n",
    "        bitcoin_sentiments.append({\n",
    "            \"text\":text,\n",
    "            \"date\":date,\n",
    "            \"compound\":compound,\n",
    "            \"positive\":pos,\n",
    "            \"neutral\":neu,\n",
    "            \"negative\":neg\n",
    "        })\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    \n",
    "bitcoin_df = pd.DataFrame(bitcoin_sentiments)\n",
    "cols = [\"date\", \"text\", \"compound\", \"positive\", \"neutral\", \"negative\"]\n",
    "stock_x_df = stock_x_df[cols]\n",
    "stock_x_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1efe10-2def-493c-a9d0-cc0dffc508e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the stock_x Sentiment\n",
    "\n",
    "stock_x_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fb8b50-cee9-4ce3-a1c9-35bb8edc91a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_x_df[\"positive\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9936cc-8b80-4f4b-85bc-8f1ecef95111",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_x_df[\"compound\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0488a6-f48d-4e09-98c6-d5721fc3484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_x_df[\"compound\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c9c9f2-917a-4551-be4e-b41e623f61cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from string import punctuation\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b559e6-4658-41f5-9e85-4f6c8c07f02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create a list of stopwords\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "# Expand the default stopwords list if necessary\n",
    "\n",
    "sw_addon={'char', 'reuters', 'ha', 'cryptocurrency', 'million','â€¦'}\n",
    "\n",
    "\n",
    "sw=sw.union(sw_addon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7569419a-2920-421f-a06b-46a4d28371ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete the tokenizer function\n",
    "def tokenizer(text):\n",
    "    \"\"\"Tokenizes text.\"\"\"\n",
    "   \n",
    "    # Remove the punctuation from text\n",
    "    for word in text:\n",
    "        if word in punctuation:\n",
    "            text = text.replace(word, \"\")\n",
    "   \n",
    "    # Create a tokenized list of the words\n",
    "    words = word_tokenize(text)   \n",
    "    \n",
    "    # Lemmatize words into root words\n",
    "    lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "   \n",
    "    # Convert the words to lowercase & Remove the stopwords\n",
    "    tokens = [word.lower() for word in lem if word.lower() not in sw]    \n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85f119-5203-43d2-afb6-d352de9e63b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new tokens column for stock_x\n",
    "bitcoin_tokens = []\n",
    "\n",
    "for i in range(len(stock_x_df)):\n",
    "    try:\n",
    "        stock_x_tokens.append(tokenizer(stock_x_df['text'][i]))\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "stock_x_df['tokens'] = stock_x_tokens\n",
    "stock_x_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b8560f-1f9e-451b-a1eb-fb7a30f7d0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbbb852-4b5d-45b0-bfe5-c53df9cc819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(doc):\n",
    "    sw = set(stopwords.words('english'))\n",
    "    regex = re.compile(\"[^a-zA-Z ]\")\n",
    "    re_clean = regex.sub('', article)\n",
    "    words = word_tokenize(re_clean)\n",
    "    lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "    output = [word.lower() for word in lem if word.lower() not in sw]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ebe81-7337-47d4-8fe4-a57cbfe99dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the counter function\n",
    "def word_counter(corpus): \n",
    "    # Combine all articles in corpus into one large string\n",
    "    big_string = ' '.join(corpus)\n",
    "    processed = process_text(big_string)\n",
    "    top_10 = dict(Counter(processed).most_common(10))\n",
    "    return pd.DataFrame(list(top_10.items()), columns=['word', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d74c4d-d81d-49c3-8f68-4e96e19c7799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the stock x N-grams where N=2\n",
    "\n",
    "n=2\n",
    "bigram_counts_xc = []\n",
    "for i in range(len(stock_x_df['text'])):\n",
    "    bigram_counts_xc.append(Counter(ngrams(word_tokenize(stock_x_df['text'][i]), n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c128216-6984-4e3e-9380-9045b4a17b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the stock x N-grams where N=2\n",
    "# YOUR CODE HERE!\n",
    "def bigram_counter(corpus): \n",
    "#    Combine all articles in corpus into one large string\n",
    "    big_string = ' '.join(corpus)\n",
    "    processed = process_text(big_string)\n",
    "    bigrams = ngrams(processed, n=2)\n",
    "    top_10 = dict(Counter(bigrams).most_common(10))\n",
    "    return pd.DataFrame(list(top_10.items()), columns=['bigram', 'count'])\n",
    "n=2\n",
    "bigram_counts_stock_x = []\n",
    "for i in range(len(stock_x_df['text'])):\n",
    "    bigram_counts_eth.append(Counter(ngrams(word_tokenize(stock_x_df['text'][i]), n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adad4832-7300-4b93-9811-5f562505b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad39608-343f-40d4-8335-adde86ca4510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a08926-5c15-4431-b528-90f502748839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the language model for SpaCy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb8989-9a54-4b7c-9915-cd83242ca1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7d62bd-b0ed-4a29-8cc7-eb46f98db8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58dfc16-3e60-4dd5-ae81-74e47a3040c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all of the stock x text together\n",
    "stock_x_text = ''\n",
    "for text in stock_x_df['text']:\n",
    "    stock_x_text += str(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b45495b-acb8-461c-ac7f-e1db5c6bbae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the NER processor on all of the text\n",
    "stock_x_ner = nlp(stock_x_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c85a1c7-a320-4eb4-9ed9-8693dbab1072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render the visualization\n",
    "displacy.render(stock_x_ner, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630ba6dd-d89a-4d2e-a938-257fb341c86a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bd57de-9f86-4280-a133-a12b0e2e0da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102379f9-feb7-4914-b83e-57b236b64423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cb07d0-567d-4e4f-a701-999bb51454b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
